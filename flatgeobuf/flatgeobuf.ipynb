{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlatGeobuf example\n",
    "\n",
    "This notebook will give an overview of how to read and write FlatGeobuf files with GeoPandas, putting an emphasis on cloud-native operations where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary way to interact with FlatGeobuf in Python is via bindings to GDAL, as there is no pure-Python implementation of FlatGeobuf.\n",
    "\n",
    "There are two different Python libraries for interacting between Python and GDAL's vector support: `fiona` and `pyogrio`. Both of these are integrated into [`geopandas.read_file`](https://geopandas.org/en/stable/docs/reference/api/geopandas.read_file.html) via the `engine` keyword, but each library has very different performance characteristics.\n",
    "\n",
    "[`fiona`](https://github.com/Toblerity/Fiona) is the default engine for `geopandas.read_file`. It provides full-featured bindings to GDAL, but does not implement a [_vectorized_](https://wesmckinney.com/book/numpy-basics#ndarray_binops) API, meaning that each row of the source file is read individually, looping over each row in Python instead of in compiled (faster) code. [`pyogrio`](https://github.com/geopandas/pyogrio) is a newer GDAL interface to Python that focuses on vectorized reading and writing of data. By moving the underlying for loop into C, rather than Python, `pyogrio` can achieve vast speedups (up to 40x) over `fiona`. You can opt in to using `pyogrio` with `geopandas.read_file` by passing `engine=\"pyogrio\"`.\n",
    "\n",
    "Additionally, if you're using a recent version of GDAL (GDAL version 3.6 or higher), you can pass `use_arrow=True` to `geopandas.read_file` to use `pyogrio`'s support for [GDAL's RFC 86](https://gdal.org/development/rfc/rfc86_column_oriented_api.html), which speeds up data reading even more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import geopandas as gpd\n",
    "import pyogrio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from local disk\n",
    "\n",
    "First we'll cover reading FlatGeobuf from local disk storage. As a first example, we'll use the US counties FlatGeobuf data from [this example](https://observablehq.com/@bjornharrtell/streaming-flatgeobuf). This file is only 13 MB in size, which we'll download to cover simple loading from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory in which to save the file\n",
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "# URL to download\n",
    "url = \"https://flatgeobuf.org/test/data/UScounties.fgb\"\n",
    "\n",
    "# Download, saving the output path\n",
    "local_fgb_path, _ = urlretrieve(url, f\"{tmpdir.name}/countries.fgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the cases below, we use `geopandas.read_file` to read the file into a `GeoDataFrame`.\n",
    "\n",
    "First we'll show that reading this file with `engine=\"fiona\"` (the default) is slower. Taking an extra 500 milliseconds might not seem like a lot, but this file contains only 3,000 rows, so this difference gets magnified with larger files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 ms ± 6.57 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gdf = gpd.read_file(local_fgb_path, engine=\"fiona\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing `engine=\"pyogrio\"` speeds up loading by 18x here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.9 ms ± 468 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gdf = gpd.read_file(local_fgb_path, engine=\"pyogrio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `use_arrow=True` often makes loading slightly faster again! We're now 21x faster than using fiona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ms ± 351 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gdf = gpd.read_file(local_fgb_path, engine=\"pyogrio\", use_arrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to local disk\n",
    "\n",
    "Similarly, we can use `GeoDataFrame.to_file` to write to a local FlatGeobuf file. As expected, writing can be much faster if you pass `engine=\"pyogrio\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 354 ms, sys: 32 ms, total: 386 ms\n",
      "Wall time: 394 ms\n"
     ]
    }
   ],
   "source": [
    "%time gdf.to_file(f\"{tmpdir.name}/out_fiona.fgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 ms, sys: 23.5 ms, total: 84.5 ms\n",
      "Wall time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "%time gdf.to_file(f\"{tmpdir.name}/out_pyogrio.fgb\", engine=\"pyogrio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from the cloud\n",
    "\n",
    "Knowing how to read and write local files is important, but given that FlatGeobuf is a cloud-optimized format, it's important to be able to read from cloud-hosted files as well.\n",
    "\n",
    "For this example, we'll use the EuroCrops data hosted on Source Cooperative because it has versions of the same data in both FlatGeobuf and GeoParquet format. Hopefully using the same dataset for both the FlatGeobuf and GeoParquet example notebooks will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.source.coop/cholmes/eurocrops/unprojected/flatgeobuf/FR_2018_EC21.fgb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when reading from the cloud, you want to filter on some spatial extent. Pyogrio offers a `read_info` function to access many pieces of information about the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crs': 'EPSG:4326',\n",
       " 'encoding': 'UTF-8',\n",
       " 'fields': array(['ID_PARCEL', 'SURF_PARC', 'CODE_CULTU', 'CODE_GROUP', 'CULTURE_D1',\n",
       "        'CULTURE_D2', 'EC_org_n', 'EC_trans_n', 'EC_hcat_n', 'EC_hcat_c'],\n",
       "       dtype=object),\n",
       " 'dtypes': array(['object', 'float64', 'object', 'object', 'object', 'object',\n",
       "        'object', 'object', 'object', 'object'], dtype=object),\n",
       " 'geometry_type': 'MultiPolygon',\n",
       " 'features': 9517874,\n",
       " 'driver': 'FlatGeobuf',\n",
       " 'capabilities': {'random_read': 1,\n",
       "  'fast_set_next_by_index': 0,\n",
       "  'fast_spatial_filter': 1},\n",
       " 'layer_metadata': None,\n",
       " 'dataset_metadata': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyogrio.read_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly the output of `read_info` does [not yet include](https://github.com/geopandas/pyogrio/issues/274) the bounding box of the file, even though the FlatGeobuf file contains that information in the header. This may be a reason to consider externalizing metadata using [Spatio-Temporal Asset Catalog files](https://stacspec.org/en) (STAC) in the future. For now we'll hard-code a region around Valence in the south of France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = (3.733296, 44.677061, 4.717124, 45.179431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch a dataframe containing only the records in these bounds by passing a `bbox` argument to `read_file`. Note that the Coordinate Reference System of this bounding box **must match** the CRS of the dataset. Here, we know from the output of `read_info` that the CRS of the dataset is EPSG:4326, so we can pass a longitude-latitude bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 1.16 s, total: 15.9 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time crops_gdf = gpd.read_file(url, bbox=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing `engine=\"pyogrio\"` is only slightly faster, which may mean that most of the time is taken up in network requests, not in parsing the actual data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 1.55 s, total: 5.12 s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%time crops_gdf = gpd.read_file(url, bbox=bounds, engine=\"pyogrio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a much smaller dataset of only 123,000 rows (down from 9.5 million rows in the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123549"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crops_gdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
